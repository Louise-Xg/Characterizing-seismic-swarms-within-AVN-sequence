{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Restructure import *\n",
    "from new_K_seismicity_rate3 import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from AVN import *\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import scipy.io as scp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density-based clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_df0 = \"/home/xianglo/Bureau/DATA_AMATRICE/DBSCAN_David/new_all_run_files_for_etas/catalog_all/\"\n",
    "\n",
    "####Raw data:\n",
    "df0 = pd.read_csv(path_to_df0+'raw_data_modified_version.csv')\n",
    "###Structure : t, x, y, z, m, x_km, y_km\n",
    "\n",
    "###Clusters from dbscan:\n",
    "v0_n50_d0_3 = scp.loadmat(\"dbscan_data/v0_d03km_n50_every1.mat\")[\"clusters\"].reshape(-1) ##au moins 50 séismes à < 200 m, v = 0km/d : 1 séisme sur 1 ou 5 comme vertex\n",
    "\n",
    "###Restructure:\n",
    "def restructure_dbscan(dbscan) :\n",
    "    \n",
    "    new_dbscan = []\n",
    "    for i in range(0, len(dbscan)):\n",
    "        new_dbscan.append(int(dbscan[i]))\n",
    "        \n",
    "    new_dbscan = np.reshape(new_dbscan, -1)\n",
    "    \n",
    "    return new_dbscan\n",
    "\n",
    "v0_n50_d0_3 = restructure_dbscan(v0_n50_d0_3)\n",
    "\n",
    "df0[\"v0_n50_d0_3\"] = v0_n50_d0_3\n",
    "\n",
    "###Take into account m0:\n",
    "m0 = -5\n",
    "df0 = df0[df0.m >= m0]\n",
    "\n",
    "###Remove all rows with a magnitude equal to a nan value:\n",
    "df0 = df0.dropna()\n",
    "\n",
    "df0.index = np.arange(0, len(df0))\n",
    "\n",
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Define AVN MS:\n",
    "A, V, N, df_AVN = AVN(df0)\n",
    "\n",
    "df_AVN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Define colors code for plots:\n",
    "from matplotlib import cm\n",
    "\n",
    "##Get colors : \n",
    "cmap_name1 = cm.get_cmap('tab20', 20)\n",
    "cmap_name2 = cm.get_cmap('tab20b', 20)\n",
    "cmap_name3 = cm.get_cmap('tab20c', 20)\n",
    "cmap_name4 = cm.get_cmap('tab10', 10)\n",
    "cmap_name5 = cm.get_cmap('Set3', 12)\n",
    "\n",
    "cmap1 = cmap_name1(np.linspace(0, 1, 20))\n",
    "cmap2 = cmap_name2(np.linspace(0, 1, 20))\n",
    "cmap3 = cmap_name3(np.linspace(0, 1, 20))\n",
    "cmap4 = cmap_name4(np.linspace(0, 1, 10))\n",
    "cmap5 = cmap_name5(np.linspace(0, 1, 12))\n",
    "\n",
    "cmap = np.vstack([cmap1, cmap2, cmap3, cmap4, cmap5, \n",
    "                  cmap1, cmap2, cmap3, cmap4, cmap5, \n",
    "                  cmap1, cmap2, cmap3, cmap4, cmap5,\n",
    "                  cmap1, cmap2, cmap3, cmap4, cmap5,\n",
    "                  cmap1, cmap2, cmap3, cmap4, cmap5])\n",
    "print(len(cmap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inversion ETAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output files obtained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "###################CHECK SWARMS###################\n",
    "\n",
    "#Organizing files:\n",
    "path = \"/home/xianglo/Bureau/DATA_AMATRICE/DBSCAN_David/new_all_run_files_for_etas/v0_d03_n50_every1_clusters/output_dbscan_v0_d03_n50_every1/\"\n",
    "path_coords = \"/home/xianglo/Bureau/DATA_AMATRICE/DBSCAN_David/new_all_run_files_for_etas/v0_d03_n50_every1_clusters/dbscan_parameter_files_v0_d03_n50_every1/\"\n",
    "\n",
    "count = 0\n",
    "count_step1 = 0\n",
    "get_cluster_num = []\n",
    "\n",
    "##for final table of swarms saved:\n",
    "all_t1_swarms = []\n",
    "all_t2_swarms = []\n",
    "all_x1_swarms = []\n",
    "all_x2_swarms = []\n",
    "all_y1_swarms = []\n",
    "all_y2_swarms = []\n",
    "all_z1_swarms = []\n",
    "all_z2_swarms = []\n",
    "\n",
    "all_select_ratio_swarms = []\n",
    "\n",
    "##Parameter:\n",
    "ratio_factor = 10\n",
    "time_factor = 5 ##d\n",
    "min_cluster_size = 50 ##same as when running DBSCAN\n",
    "\n",
    "for num_cluster_HDBSCAN in range(1, df0[\"v0_n50_d0_3\"].max()+1) :\n",
    "   \n",
    "    df2 = np_to_pd_3(path+\"dbscan_clust\"+str(num_cluster_HDBSCAN)+\".dat\")\n",
    "\n",
    "    ###We are interest in the cumulative seismic events first:\n",
    "    df00 = df2[df2.ind == 1]\n",
    "    df1 = np.cumsum(df00.ind)\n",
    "\n",
    "    ###Compute seismicty rate: \n",
    "    dn = int(np.ceil(len(df00)*5/100)) ##smooth windown ##taking 5% of cluster's data\n",
    "\n",
    "    ###Observed data: \n",
    "    dff = df1\n",
    "    EQ1_rate = seismicity_rate2 (dn, df00, dff)\n",
    "\n",
    "    ###The model data:\n",
    "    dff = df00.lam\n",
    "    EQ2_rate = seismicity_rate2 (dn, df00, dff)\n",
    "\n",
    "    ##obs/mod:\n",
    "    ratio = EQ1_rate/EQ2_rate\n",
    "\n",
    "    arg = np.argwhere((ratio >= ratio_factor)&(ratio != np.inf))\n",
    "    arg = arg.T[0]\n",
    "\n",
    "    if len(arg) >= min_cluster_size:\n",
    "\n",
    "        ##time data:\n",
    "        tps = df00.t.values\n",
    "\n",
    "        ##label of swarms:\n",
    "        label = np.zeros(len(arg))\n",
    "\n",
    "        ##diff in time for only data which ratio >= ratio_factor:\n",
    "        dt = tps[arg[1:]] - tps[arg[:-1]]\n",
    "        max_dt = np.max(dt)\n",
    "\n",
    "        ##Find the beginning of the swarms:\n",
    "        arg2 = np.argwhere(dt >= time_factor)        \n",
    "        arg2 = arg2.T[0] +1\n",
    "        label[arg2] += 1\n",
    "        label[0] = 1\n",
    "\n",
    "        ##Determine swarms:\n",
    "        cumsum = np.cumsum(label)\n",
    "\n",
    "        ##create a table:\n",
    "        data = {\"t\":tps, \"lam_mod\":EQ2_rate, \"lam_obs\":EQ1_rate, \"ratio\":ratio, \"label\": np.zeros(len(ratio))-1}\n",
    "        df_swarms = pd.DataFrame(data)\n",
    "\n",
    "        ##put the right label of swarms:\n",
    "        df_swarms.label.iloc[arg] = cumsum\n",
    "\n",
    "        ##select swarms name:\n",
    "        swarms_label = np.unique(df_swarms.label)[1:]\n",
    "\n",
    "        for i in swarms_label:\n",
    "            select_swarms = df_swarms[df_swarms.label == i]\n",
    "            swarm_size = len(select_swarms)\n",
    "            \n",
    "            count_step1 +=1\n",
    "\n",
    "            if (swarm_size >= min_cluster_size) == True:\n",
    "\n",
    "                count += 1\n",
    "\n",
    "                ###Get spatial coords:\n",
    "                param_file_name = path_coords+\"dbscan_cluster\"+str(num_cluster_HDBSCAN)\n",
    "                with open(param_file_name) as f:\n",
    "                    lines = f.readlines()\n",
    "                    lines_coords = lines[2].split()\n",
    "                    x1, x2, y1, y2, z1, z2 = float(lines_coords[0]), float(lines_coords[1]), float(lines_coords[2]), float(lines_coords[3]) , float(lines_coords[4]), float(lines_coords[5]) \n",
    "                    ##x:lat(°), y:lon(°), z:z(km)\n",
    "\n",
    "                all_x1_swarms.append(x1) ##deg \n",
    "                all_x2_swarms.append(x2) ##deg\n",
    "                all_y1_swarms.append(y1) ##deg\n",
    "                all_y2_swarms.append(y2) ##deg\n",
    "                all_z1_swarms.append(z1) ##km\n",
    "                all_z2_swarms.append(z2) ##km\n",
    "\n",
    "                get_cluster_num.append(num_cluster_HDBSCAN)\n",
    "\n",
    "                all_t1_swarms.append(select_swarms.t.min()) ##d\n",
    "                all_t2_swarms.append(select_swarms.t.max()) ##d\n",
    "\n",
    "                ##Plot ratio:\n",
    "                plt.figure(figsize=(15,5))\n",
    "                plt.plot(df00.t,ratio)\n",
    "                plt.xlabel(\"Time (d)\", fontsize=20)\n",
    "                plt.ylabel(\"Ratio (obs/mod)\", fontsize=20)\n",
    "                plt.axhline(y=ratio_factor, linewidth=2, color='purple', ls = '--')\n",
    "                for j in swarms_label:\n",
    "                    each_swarms = df_swarms[df_swarms.label == j]\n",
    "                    swarm_size = len(each_swarms)\n",
    "\n",
    "                    if swarm_size >= min_cluster_size:\n",
    "                        plt.axvline(each_swarms.t.min(), c=cmap[int(j)], linestyle='--')\n",
    "                        plt.axvline(each_swarms.t.max(), c=cmap[int(j)], linestyle='--')\n",
    "                        plt.xlim(each_swarms.t.min()-1, each_swarms.t.max()+1)\n",
    "                        \n",
    "                        ##get ratio data:\n",
    "                        select_ind = df_swarms.t[(df_swarms.t >=each_swarms.t.min()) & (df_swarms.t <= each_swarms.t.max())]\n",
    "                        select_ind1 = select_ind.index[0]\n",
    "                        select_ind2 = select_ind.index[-1]\n",
    "                        select_ratio = ratio[select_ind1:select_ind2]\n",
    "                        all_select_ratio_swarms.append(select_ratio)\n",
    "                        print(j, select_ind1, select_ind2)\n",
    "                        \n",
    "                plt.title('cluster n°'+str(num_cluster_HDBSCAN)+' (HDBSCAN), mP=2, v=0km/d, ratio_factor ='+str(ratio_factor)+\", time_factor =\" +str(time_factor)+\"d, dn =\"+str(dn), fontsize=15)\n",
    "                plt.xticks(fontsize=15,)\n",
    "                plt.yticks(fontsize=15,)\n",
    "                plt.grid(True)\n",
    "                #plt.savefig(\"/home/xianglo/Bureau/DATA_AMATRICE/DBSCAN_David/new_all_run_files_for_etas/v0_d03_n50_every1_clusters/ratio_for_clusters/dbscan\"+str(num_cluster_HDBSCAN)+\"_zoom.png\")\n",
    "                plt.show()\n",
    "\n",
    "                ###Plot of swarms:\n",
    "                plt.figure(figsize=(15,5))\n",
    "                plt.plot(df_swarms.t, EQ1_rate, '-k', label = \"observed data\")\n",
    "                plt.plot(df_swarms.t, EQ2_rate, c = \"crimson\", label = \"predicted data\")\n",
    "\n",
    "                for j in swarms_label:\n",
    "                    each_swarms = df_swarms[df_swarms.label == j]\n",
    "                    swarm_size = len(each_swarms)\n",
    "\n",
    "                    if swarm_size >= min_cluster_size:\n",
    "                        \n",
    "                        plt.axvline(each_swarms.t.min(), c=cmap[int(j)], linestyle='--')\n",
    "                        plt.axvline(each_swarms.t.max(), c=cmap[int(j)], linestyle='--')\n",
    "                        plt.xlim(each_swarms.t.min()-1, each_swarms.t.max()+1)\n",
    "                        \n",
    "                plt.title('cluster n°'+str(num_cluster_HDBSCAN)+' (HDBSCAN), mP=2, v=0km/d, ratio_factor ='+str(ratio_factor)+\", time_factor =\" +str(time_factor)+\"d, dn =\"+str(dn), fontsize=15)\n",
    "                plt.xlabel(\"Time (d)\", fontsize=20)\n",
    "                plt.ylabel(\"Seismicity rate\", fontsize=20)\n",
    "                plt.yscale(\"log\")\n",
    "                plt.xticks(fontsize=15,)\n",
    "                plt.yticks(fontsize=15,)\n",
    "                plt.grid(True)\n",
    "                plt.legend(fontsize=15)\n",
    "                #plt.savefig(\"/home/xianglo/Bureau/DATA_AMATRICE/DBSCAN_David/new_all_run_files_for_etas/v0_d03_n50_every1_clusters/SR_for_clusters/dbscan\"+str(num_cluster_HDBSCAN)+\"_zoom.png\")\n",
    "                plt.show()\n",
    "\n",
    "##create final table of swarms:a\n",
    "data = {\"t1\":all_t1_swarms, \"t2\":all_t2_swarms,\n",
    "       \"x1\":all_x1_swarms, \"x2\":all_x2_swarms,\n",
    "       \"y1\":all_y1_swarms, \"y2\":all_y2_swarms,\n",
    "       \"z1\":all_z1_swarms, \"z2\":all_z2_swarms,\n",
    "       \"num_EQ\":get_cluster_num}\n",
    "\n",
    "df_final_swarms = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_select_ratio_swarms) ##y a les doublons/triplons == 10 d'où 51 au lieu des 41\n",
    "#all_select_ratio_swarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_mean = []\n",
    "all_Q2 = []\n",
    "all_Q3 = []\n",
    "all_min = []\n",
    "all_max = []\n",
    "\n",
    "#for i in range(0, 4):\n",
    "#\n",
    "for i in range(0, len(all_select_ratio_swarms)):\n",
    "    a = all_select_ratio_swarms[i]\n",
    "    a_mean = np.mean(a)\n",
    "    a_Q2 = np.percentile(a, 50)  ##median\n",
    "    a_Q3 = np.percentile(a, 75)  \n",
    "    a_min = np.min(a)\n",
    "    a_max = np.max(a)\n",
    "    all_mean.append(a_mean)\n",
    "    all_Q2.append(a_Q2)\n",
    "    all_Q3.append(a_Q3)\n",
    "    all_min.append(a_min)\n",
    "    all_max.append(a_max)\n",
    "\n",
    "df_ratio = pd.DataFrame({\"mean\":all_mean, \"Q2\":all_Q2, \"Q3\":all_Q3, \"min\":all_min, \"max\":all_max})\n",
    "df_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##On retire les 10 doublons/triplons:\n",
    "df_ratio = df_ratio.drop_duplicates(subset=[\"mean\", \"Q2\", \"Q3\", \"min\", \"max\"], keep='first', inplace=False, ignore_index=False)\n",
    "print(len(df_ratio))\n",
    "df_ratio.index = np.arange(0, len(df_ratio))\n",
    "\n",
    "df_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##On crée un datframe qui prend aussi les valeurs de ratio:\n",
    "df_final_swarms_ratio = pd.concat([df_final_swarms, df_ratio], axis=1)\n",
    "\n",
    "# df_final_swarms_ratio = df_final_swarms_ratio.sort_values(\"t1\")\n",
    "# df_final_swarms_ratio.index = np.arange(1, len(df_final_swarms_ratio)+1)\n",
    "\n",
    "# df_final_swarms_ratio.to_csv(\"/home/xianglo/Bureau/DATA_AMATRICE/DBSCAN_David/new_all_run_files_for_etas/v0_d03_n50_every1_clusters/df_final_swarms_of_v0_d03_n50_every1_with_ratio.csv\", index = False)\n",
    "\n",
    "df_final_swarms_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final_swarms = df_final_swarms.sort_values(\"t1\")\n",
    "df_final_swarms.index = np.arange(1, len(df_final_swarms)+1)\n",
    "\n",
    "#df_final_swarms.to_csv(\"v0_d03_n50_every5_clusters/df_final_swarms_of_v0_d03_n50_every1.csv\", index = False)\n",
    "\n",
    "df_final_swarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
